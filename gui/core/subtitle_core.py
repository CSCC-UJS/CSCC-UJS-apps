# import os,re
# import sys
# import time
# import tempfile
# from PySide6.QtCore import QObject, Signal
# from datetime import timedelta
# import whisper
# from opencc import OpenCC
# import subprocess
# from tqdm import tqdm
# from gui.core.api_utils.asr import ASRClient
# # å­—å¹•ç”Ÿæˆ
# # éŸ³é¢‘åˆ‡ç‰‡ï¼Œå¯ä»¥æ­£å¸¸è¯†åˆ«éŸ³é¢‘å¹¶è¾“å‡ºåˆ†ç‰‡å­—å¹•
# def clean_temp(files):
#     for file in files:
#         if os.path.exists(file):
#             try:
#                 os.remove(file)
#             except Exception as e:
#                 print(f"è­¦å‘Šï¼šæ— æ³•åˆ é™¤ä¸´æ—¶æ–‡ä»¶ {file}: {e}")

# class SubtitleWorker(QObject):
#     progress = Signal(str, int)
#     finished = Signal(bool, str)
#     subtitle_updated = Signal(float,float,str)  

#     def __init__(self,asr_client, video_path, srt_path,  total_duration, slice_duration=10, short_segment_threshold=2.0):
#         super().__init__()
#         self.asr_client=asr_client
#         self.video_path = video_path
#         self.srt_path = srt_path,
#         self.slice_duration = slice_duration  # åˆ‡ç‰‡æ—¶é•¿
#         self.short_segment_threshold = short_segment_threshold  # é˜ˆå€¼
#         self.is_running = True
#         self.converter = OpenCC('t2s')  
#         self.model = None  
#         self.subtitle_index = 1  
#         self.temp_files = []  
#         self.short_segments_cache = []  
#         self.total_duration=total_duration

#     def preload_model(self):
#         # """é¢„åŠ è½½Whisperæ¨¡å‹ï¼ˆé¿å…å®æ—¶è¯†åˆ«æ—¶å¡é¡¿ï¼‰"""
#         # self.progress.emit("æ­£åœ¨åŠ è½½è¯†åˆ«æ¨¡å‹...", 5)
#         # self.model = whisper.load_model(self.model_size)
#         # self.progress.emit("æ¨¡å‹åŠ è½½å®Œæˆ", 10)
#         """é¢„åŠ è½½API"""
#         self.progress.emit("æ­£åœ¨åŠ è½½è¯†åˆ«æ¨¡å‹...", 5)
#     def extract_audio_slice(self, start_time, duration, output_file):
#         cmd = [
#             'ffmpeg',
#             '-y',  # è¦†ç›–å·²å­˜åœ¨çš„æ–‡ä»¶
#             '-ss', f'{start_time:.2f}',  # èµ·å§‹æ—¶é—´
#             '-i', self.video_path,  # è¾“å…¥è§†é¢‘æ–‡ä»¶
#             '-t', f'{duration:.2f}',  # æå–æ—¶é•¿
#             '-vn',  # ç¦ç”¨è§†é¢‘æµ
#             '-acodec', 'pcm_s16le',  # éŸ³é¢‘ç¼–ç 
#             '-ar', '16000',  # é‡‡æ ·ç‡
#             '-ac', '1',  # å•å£°é“
#             '-f', 'wav',  # è¾“å‡ºæ ¼å¼
#             '-hide_banner',  # éšè—bannerä¿¡æ¯
#             '-loglevel', 'error',  # ä»…è¾“å‡ºé”™è¯¯ä¿¡æ¯
#             output_file  # è¾“å‡ºæ–‡ä»¶è·¯å¾„
#         ]

#         try:
#             subprocess.run(
#                 cmd,
#                 check=True,
#                 stdout=subprocess.PIPE,
#                 stderr=subprocess.PIPE,
#                 encoding='utf-8'
#             )
#         except subprocess.CalledProcessError as e:
#             print(f"éŸ³é¢‘æå–å¤±è´¥ï¼š{e.stderr}")
#             raise Exception(f"ffmpegæå–éŸ³é¢‘å¤±è´¥ï¼š{e.stderr}")

#         if not os.path.exists(output_file) or os.path.getsize(output_file) == 0:
#             raise Exception(f"éŸ³é¢‘åˆ‡ç‰‡æ–‡ä»¶ç”Ÿæˆå¤±è´¥ï¼š{output_file}")


#     def merge_short_segments(self):
#         if not self.short_segments_cache:
#             return
        
#         merged_start = self.short_segments_cache[0]["start"]
#         merged_end = self.short_segments_cache[-1]["end"]
#         merged_text = "".join([seg["text"] for seg in self.short_segments_cache])
        
#         start_str = self.format_srt_time(merged_start)
#         end_str = self.format_srt_time(merged_end)
        
#         with open(self.srt_path, "a", encoding="utf-8") as f:
#             f.write(f"{self.subtitle_index}\n")
#             f.write(f"{start_str} --> {end_str}\n")
#             f.write(f"{merged_text}\n\n")
        
#         self.subtitle_updated.emit(merged_start,merged_end,merged_text)
#         self.subtitle_index += 1
        
#         self.short_segments_cache = []

#     def run(self):
#         try:
#             self.preload_model()

#             if not self.total_duration or self.total_duration <= 0:
#                 raise Exception("æ— æ³•è·å–è§†é¢‘æ—¶é•¿")

#             with open(self.srt_path, "w", encoding="utf-8") as f:
#                 f.write("")

#             current_time = 0
#             self.progress.emit("å¼€å§‹å®æ—¶ç”Ÿæˆå­—å¹•...", 10)
            
#             # è®¡ç®—æ€»åˆ‡ç‰‡æ•°
#             total_slices = int(self.total_duration / self.slice_duration) + 1
            
#             # ä½¿ç”¨tqdmæ˜¾ç¤ºè¿›åº¦æ¡ï¼ˆåŒæ—¶æŒ‡å®šfileå’Œpositioné¿å…é‡å¤ï¼‰
#             with tqdm(total=total_slices, desc="å­—å¹•ç”Ÿæˆ", unit="åˆ‡ç‰‡", ncols=100, file=sys.stdout, position=0) as pbar:
#                 while current_time < self.total_duration and self.is_running:
#                     # è®¡ç®—å½“å‰è¿›åº¦
#                     progress_percent = int(10 + (current_time / self.total_duration) * 90)
#                     self.progress.emit(f"æ­£åœ¨è¯†åˆ« {current_time:.1f} - {current_time + self.slice_duration:.1f} ç§’...", progress_percent)

#                     # åˆ›å»ºä¸´æ—¶éŸ³é¢‘åˆ‡ç‰‡æ–‡ä»¶
#                     temp_fd, temp_audio = tempfile.mkstemp(suffix=".wav")
#                     os.close(temp_fd)  # å…³é—­æ–‡ä»¶æè¿°ç¬¦
#                     self.temp_files.append(temp_audio)

#                     # æå–å½“å‰æ—¶é—´æ®µçš„éŸ³é¢‘åˆ‡ç‰‡
#                     self.extract_audio_slice(current_time, self.slice_duration, temp_audio)

#                     # æ£€æŸ¥åˆ‡ç‰‡æ–‡ä»¶æ˜¯å¦æœ‰æ•ˆ
#                     if not os.path.exists(temp_audio) or os.path.getsize(temp_audio) == 0:
#                         current_time += self.slice_duration
#                         continue

#                     #è¯†åˆ«åˆ‡ç‰‡éŸ³é¢‘
#                     # result = self.model.transcribe(
#                     #     temp_audio,
#                     #     language="zh",
#                     #     verbose=None,  # ä½¿ç”¨Noneå®Œå…¨ç¦ç”¨è¾“å‡º
#                     #     fp16=False,
#                     #     no_speech_threshold=0.1
#                     # )
#                     res = self.asr_client.transcribe(
#                         file_path=temp_audio,
#                         model="TeleAI/TeleSpeechASR"
#                     )
#                     #ä¹‹å‰å†™æ­»å¼ºåˆ¶ä»¥slice_durationä¸ºå­—å¹•èµ·æ­¢æ—¶é—´é—´éš”
#                     # è§£æè¯†åˆ«ç»“æœå¹¶å¤„ç†çŸ­ç‰‡æ®µåˆå¹¶
#                     if result["segments"]:
#                         for seg in result["segments"]:
#                             # ä¿®æ­£å­—å¹•æ—¶é—´ï¼ˆåŸºäºåˆ‡ç‰‡èµ·å§‹æ—¶é—´ï¼‰
#                             seg_start = current_time + seg["start"]
#                             seg_end = current_time + seg["end"]
#                             seg_duration = seg_end - seg_start
#                             # ç¹è½¬ç®€
#                             subtitle_text = self.converter.convert(seg["text"].strip())

#                             # åˆ¤æ–­æ˜¯å¦ä¸ºçŸ­ç‰‡æ®µ
#                             if seg_duration < self.short_segment_threshold:
#                                 # åŠ å…¥ç¼“å­˜
#                                 self.short_segments_cache.append({
#                                     "start": seg_start,
#                                     "end": seg_end,
#                                     "text": subtitle_text
#                                 })
#                             else:
#                                 # å…ˆåˆå¹¶ç¼“å­˜ä¸­çš„çŸ­ç‰‡æ®µ
#                                 self.merge_short_segments()
#                                 # ç›´æ¥å†™å…¥é•¿ç‰‡æ®µ
#                                 start_str = self.format_srt_time(seg_start)
#                                 end_str = self.format_srt_time(seg_end)
#                                 with open(self.srt_path, "a", encoding="utf-8") as f:
#                                     f.write(f"{self.subtitle_index}\n")
#                                     f.write(f"{start_str} --> {end_str}\n")
#                                     f.write(f"{subtitle_text}\n\n")
#                                 self.subtitle_updated.emit(seg_start,seg_end,subtitle_text)
#                                 self.subtitle_index += 1

#                     # åˆ‡ç‰‡ç»“æŸæ—¶ï¼Œåˆå¹¶å‰©ä½™çš„çŸ­ç‰‡æ®µ
#                     self.merge_short_segments()
#                     # æ¨è¿›æ—¶é—´è½´
#                     current_time += self.slice_duration
#                     time.sleep(0.1)
                        
#                     # æ›´æ–°è¿›åº¦æ¡
#                     pbar.update(1)
#                     pbar.set_postfix_str(f"æ—¶é—´: {current_time:.1f}s")

#                 # æœ€ç»ˆåˆå¹¶æ‰€æœ‰å‰©ä½™çš„çŸ­ç‰‡æ®µ
#                 self.merge_short_segments()

#                 if self.is_running:
#                     self.progress.emit("å®æ—¶å­—å¹•ç”Ÿæˆå®Œæˆï¼", 100)
#                     self.finished.emit(True, f"å®æ—¶å­—å¹•å·²ä¿å­˜ï¼š{os.path.basename(self.srt_path)}")
#                 else:
#                     self.finished.emit(False, "ç”¨æˆ·ç»ˆæ­¢äº†å®æ—¶å­—å¹•ç”Ÿæˆ")

#         except Exception as e:
#             self.finished.emit(False, f"å®æ—¶å­—å¹•ç”Ÿæˆå¤±è´¥ï¼š{str(e)}")
#         finally:
#             clean_temp(self.temp_files)

#     def format_srt_time(self, seconds):
#         """å°†ç§’æ•°è½¬æ¢ä¸ºSRTæ ‡å‡†æ—¶é—´æ ¼å¼ï¼ˆ00:00:00,000ï¼‰"""
#         td = timedelta(seconds=seconds)
#         hours, remainder = divmod(td.seconds, 3600)
#         minutes, secs = divmod(remainder, 60)
#         milliseconds = td.microseconds // 1000
#         return f"{hours:02d}:{minutes:02d}:{secs:02d},{milliseconds:03d}"

#     def stop(self):
#         self.is_running = False


import os,re
import sys
import time
import tempfile
from PySide6.QtCore import QObject, Signal
from datetime import timedelta
import whisper
from opencc import OpenCC
import subprocess
from tqdm import tqdm
from funasr import AutoModel

# å­—å¹•ç”Ÿæˆ
# éŸ³é¢‘ä¸åˆ‡ç‰‡
def clean_temp(files):

    for file in files:
        if os.path.exists(file):
            try:
                os.remove(file)
            except Exception as e:
                print(f"è­¦å‘Šï¼šæ— æ³•åˆ é™¤ä¸´æ—¶æ–‡ä»¶ {file}: {e}")


class SubtitleWorker(QObject):
    progress = Signal(str, int)
    finished = Signal(bool, str)

    def __init__(self,model, video_path, srt_path):
        super().__init__()
        self.video_path = video_path
        self.srt_path = srt_path
        self.is_running = True
        self.converter=OpenCC('t2s')
        self.model=model
    def run(self):
        audio_temp = "temp_audio.wav"
        try:
            self.progress.emit("æ­£åœ¨æå–éŸ³é¢‘...", 20)
            # æå–éŸ³é¢‘
            os.system(f'ffmpeg -i "{self.video_path}" -vn -acodec pcm_s16le -ar 16000 -ac 1 "{audio_temp}"')
            # æ£€æµ‹éŸ³é¢‘ä¸´æ—¶æ–‡ä»¶æ˜¯å¦ç”Ÿæˆï¼Œä¸”æœ‰æ•°æ®å†™å…¥
            if not self.is_running or not os.path.exists(audio_temp) or os.path.getsize(audio_temp) == 0:
                raise Exception("éŸ³é¢‘æå–å¤±è´¥æˆ–ç”¨æˆ·å–æ¶ˆæ“ä½œ")

            self.progress.emit(f"æ­£åœ¨ä½¿ç”¨ funasr æ¨¡å‹è¯†åˆ«...", 40)

            self.audio_to_subtitle(self.model,audio_temp,self.srt_path)
            if not self.is_running:
                raise Exception("ç”¨æˆ·å–æ¶ˆäº†ç”Ÿæˆ")

            self.progress.emit("æ­£åœ¨è§£æå¹¶ä¿å­˜å­—å¹•...", 80)
        
            # æç¤ºå­—å¹•ç”Ÿæˆå®Œæˆ
            self.progress.emit("å­—å¹•ç”Ÿæˆå®Œæˆï¼", 100)
            self.finished.emit(True, f"å­—å¹•å·²ä¿å­˜ä¸ºSRTæ–‡ä»¶ï¼š{os.path.basename(self.srt_path)}")

        except Exception as e:
            self.finished.emit(False, f"å­—å¹•ç”Ÿæˆå¤±è´¥ï¼š{str(e)}")
        finally:
            clean_temp([audio_temp])

    def format_time(self,seconds):
        """å°†ç§’æ•°æ ¼å¼åŒ–ä¸º SRT å­—å¹•æ—¶é—´æ ¼å¼: 00:00:00,000"""
        hours = int(seconds // 3600)
        minutes = int((seconds % 3600) // 60)
        secs = int(seconds % 60)
        ms = int((seconds - int(seconds)) * 1000)
        return f"{hours:02d}:{minutes:02d}:{secs:02d},{ms:03d}"

    def split_text_by_punctuation(self,text, split_chars=None):
        """æŒ‰æ ‡ç‚¹æ‹†åˆ†æ–‡æœ¬ï¼Œé€‚é…æ—¶é—´æˆ³åˆ†æ®µ"""
        if split_chars is None:
            split_chars = r'ï¼Œã€‚ï¼ï¼Ÿï¼›ï¼šã€.?!;:'
        parts = re.split(f'([{split_chars}])', text)
        merged_parts = []
        temp = ""
        for part in parts:
            if part:
                temp += part
                if part in split_chars:
                    merged_parts.append(temp.strip())
                    temp = ""
        if temp:
            merged_parts.append(temp.strip())
        return merged_parts

    # ==================== å¤ç”¨æ¨¡å‹ç”Ÿæˆå­—å¹• ====================
    def audio_to_subtitle(self,preloaded_model, audio_path, output_srt_path=None):

        # è®¾ç½®é»˜è®¤è¾“å‡ºè·¯å¾„
        if output_srt_path is None:
            base_name = os.path.splitext(audio_path)[0]
            output_srt_path = f"{base_name}.srt"
        
        # å¤ç”¨æ¨¡å‹è¿›è¡Œæ¨ç†ï¼ˆæ— éœ€é‡æ–°åŠ è½½ï¼‰
        print(f"\næ­£åœ¨å¤„ç†éŸ³é¢‘: {audio_path}")
        res = preloaded_model.generate(
            input=audio_path,
            batch_size_s=30,
            merge_vad=True,
            use_itn=True,
            add_pause=True,
            predict_timestamp=True
        )
        
        srt_content = []
        index = 1
        try:
            full_text = res[0].get("text", "").strip()
            timestamps_ms = res[0].get("timestamp", [])
            
            if not full_text or not timestamps_ms:
                raise ValueError("æœªè·å–åˆ°æœ‰æ•ˆæ–‡æœ¬æˆ–æ—¶é—´æˆ³")
            
            # æ‹†åˆ†æ–‡æœ¬ + åŒ¹é…æ—¶é—´æˆ³
            text_segments = self.split_text_by_punctuation(full_text)
            # é€‚é…æ–‡æœ¬å’Œæ—¶é—´æˆ³æ•°é‡
            if len(text_segments) > len(timestamps_ms):
                text_segments = text_segments[:len(timestamps_ms)]
            elif len(text_segments) < len(timestamps_ms):
                ts_per_segment = len(timestamps_ms) // len(text_segments)
                remainder = len(timestamps_ms) % len(text_segments)
                new_timestamps = []
                current = 0
                for i in range(len(text_segments)):
                    count = ts_per_segment + (1 if i < remainder else 0)
                    count = min(count, len(timestamps_ms) - current)
                    start_ms = timestamps_ms[current][0]
                    end_ms = timestamps_ms[current + count - 1][1]
                    new_timestamps.append([start_ms, end_ms])
                    current += count
                timestamps_ms = new_timestamps
            
            # ç”Ÿæˆå­—å¹•ç‰‡æ®µ
            for i in range(min(len(text_segments), len(timestamps_ms))):
                start_ms, end_ms = timestamps_ms[i]
                start_time = start_ms / 1000.0
                end_time = end_ms / 1000.0
                text = text_segments[i]
                
                if not text or start_time >= end_time:
                    continue
                
                start_str = self.format_time(start_time)
                end_str = self.format_time(end_time)
                srt_content.extend([str(index), f"{start_str} --> {end_str}", text.strip(), ""])
                index += 1
            
            if index == 1:
                raise ValueError("æœªç”Ÿæˆæœ‰æ•ˆå­—å¹•ç‰‡æ®µ")
        
        except Exception as e:
            print(f"âš ï¸ è§£æå¤±è´¥: {e}ï¼Œå¯ç”¨å…œåº•æ–¹æ¡ˆ")
            full_text = res[0].get("text", "").strip()
            if full_text:
                srt_content = ["1", "00:00:00,000 --> 00:30:00,000", full_text, ""]
        
        # å†™å…¥æ–‡ä»¶
        with open(output_srt_path, "w", encoding="utf-8") as f:
            f.write("\n".join(srt_content))
        print(f"âœ… å­—å¹•ç”Ÿæˆå®Œæˆ: {output_srt_path}")
        print(f"ğŸ“ å…±ç”Ÿæˆ {max(index-1, 1)} æ¡å­—å¹•")

    def stop(self):
        self.is_running = False